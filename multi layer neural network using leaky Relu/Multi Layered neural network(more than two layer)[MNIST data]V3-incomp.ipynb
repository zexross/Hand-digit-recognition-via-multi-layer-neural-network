{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Relu funtion as the activation function for hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import struct as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize the Parameters   \n",
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Funtion\n",
    "def sigmoid(Z):\n",
    "    A=1/(1+np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def tanh(Z):               # Use of tanh function\n",
    "    A=np.tanh(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation of Z = W*A_previouse + B\n",
    "def linear_forward(A,W,b):\n",
    "    Z=np.dot(W,A)+b\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation the of the activation Funtion. \n",
    "#        A = Sigmoid(Z) **Need to Update**\n",
    "def linear_activation_forward(A_prev, W, b, activation_type): \n",
    "    \n",
    "    if activation_type == \"Sigmoid\":            #Use of activation funtion in hidden layer is Sigmoid funtion\n",
    "        Z = linear_forward(A_prev,W,b)\n",
    "        A = sigmoid(Z)\n",
    "    elif activation_type == \"Relu\":\n",
    "        Z = linear_forward(A_prev,W,b)\n",
    "        for i in range(0,len(Z)):\n",
    "            if Z < 0:\n",
    "                A = 0.01*Z\n",
    "            elif Z >= 0:\n",
    "                A = Z\n",
    "                   \n",
    "    return A,Z  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Cost of the overall model.\n",
    "#       Cost = Summation(Loss)/(Number of input data), where Loss= - Y*ln(Activation) - (1-Y)*ln(1 - Activation)\n",
    "def compute_cost(AL,Y):\n",
    "    data_length = Y.shape[1]\n",
    "    cost =(-1/data_length) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())  \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear_backward\n",
    "def linear_backward(dZ, A):\n",
    "    data_length = A.shape[1]\n",
    "    dW = (1/data_length)*np.dot(dZ,A.T)\n",
    "    db = (1/data_length)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    return dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear_activation_backward\n",
    "def linear_activation_backward(A, cache, Layer):\n",
    "    if Layer == \"Output Layer\":\n",
    "        Y, AL = cache\n",
    "        dZ2 = A - Y\n",
    "        dW, db = linear_backward(dZ2, AL)\n",
    "        return dZ2, dW, db\n",
    "    elif Layer == \"Hidden Layer\":\n",
    "        X, W2, dZ2 = cache\n",
    "        for i range(0,len(A)):\n",
    "            if A[i] >= 0:\n",
    "                dZ1[i] = np.dot(W2.T,dZ2)*1\n",
    "            elif A < 0:\n",
    "                dZ1 = np.dot(W2.T,dZ2)*0.01\n",
    "        dW, db = linear_backward(dZ1, X)\n",
    "        return dZ1,dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates the parameters according to the obtained gradient values\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters)//2           # number of layers in the neural network\n",
    "    \n",
    "    for l in range(L):                  # Update rule for each parameter. Use a for loop.\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main body of the Nueral Net Model which contains the procedural instructions to the execution of Nueral net model\n",
    "def L_layer_network(X,Y,learningrate,layer_dims,num_iterations,print_cost):\n",
    "    costs=[]\n",
    "    \n",
    "    #Initailise and fetch the weights through dictionary\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    L = len(parameters) // 2                           # number of layers in the neural network\n",
    "    activation = {}\n",
    "    linearsum = {}\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A = X\n",
    "        # Forward propagation\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            activation['A'+str(l)], linearsum['Z'+str(l)] = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b' + str(l)], \"Relu\")   #Calculation of activaion output of hidden layer\n",
    "            A = activation['A'+str(l)]\n",
    "        \n",
    "        activation['A'+str(L)], linearsum['Z'+str(L)] = linear_activation_forward(A, parameters['W'+str(L)], parameters['b' + str(L)], \"Sigmoid\")        #Calculation of activation output of output layer\n",
    "    \n",
    "        # Compute cost\n",
    "        cost = compute_cost(activation['A'+str(L)], Y)\n",
    "        costs.append(cost)\n",
    "        \n",
    "         # Stores the gradient values in the dictionary grads\n",
    "        grads = {}\n",
    "        data_length = Y.shape[1]\n",
    "        \n",
    "        # Backward propagation\n",
    "        activation['A'+str(0)] = X\n",
    "        cache = (Y, activation['A'+str(L-1)])\n",
    "        linearsum['dZ'+str(L)] ,grads['dW'+str(L)], grads['db'+str(L)] = linear_activation_backward(activation['A'+str(L)], cache, \"Output Layer\")\n",
    "        \n",
    "        for l in reversed(range(1,L)):\n",
    "            cache = (activation['A'+str(l-1)], parameters['W'+str(l+1)], linearsum['dZ'+str(l+1)])\n",
    "            linearsum['dZ'+str(l)], grads['dW'+str(l)], grads['db'+str(l)] = linear_activation_backward(activation['A'+str(l)], cache, \"Hidden Layer\")\n",
    "        \n",
    "    \n",
    "        # Update the parameters\n",
    "        parameters =  update_parameters(parameters, grads, learningrate)\n",
    "        \n",
    "        #Vary the learning rate\n",
    "        if(cost > 0.37):\n",
    "            learningrate = learningrate\n",
    "        elif(cost<0.37 and cost>0.2):\n",
    "            learningrate = 0.5\n",
    "        elif(cost<0.25 and cost>0.1):\n",
    "            learningrate = 0.3\n",
    "            \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "    \n",
    "    #Plot the cost funtion curve\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.show()\n",
    "    return parameters,activation,linearsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic Number :  2051\n",
      "num of images :  60000\n",
      "num of rows :  28\n",
      "num of column :  28\n",
      "Size images_array : (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Input Data(MNIST)\n",
    "\n",
    "filename = {'images' : 'E:/Handwritten digit recognisation/dataset/train-images-idx3-ubyte' ,'labels' : 'E:/Handwritten digit recognisation/dataset/train-labels-idx1-ubyte'}\n",
    "train_imagesfile = open(filename['images'],'rb')\n",
    "\n",
    "MagicNumber = st.unpack('>I',train_imagesfile.read(4))[0]\n",
    "nImg = st.unpack('>I',train_imagesfile.read(4))[0] #num of images\n",
    "nR = st.unpack('>I',train_imagesfile.read(4))[0] #num of rows\n",
    "nC = st.unpack('>I',train_imagesfile.read(4))[0] #num of column\n",
    "print(\"Magic Number : \", MagicNumber)\n",
    "print(\"num of images : \", nImg)\n",
    "print(\"num of rows : \", nR)\n",
    "print(\"num of column : \", nC)\n",
    "\n",
    "images_array = np.zeros((nImg,nR,nC))\n",
    "\n",
    "nBytesTotal = nImg*nR*nC*1 #since each pixel data is 1 byte\n",
    "images_array = np.asarray(st.unpack('>'+'B'*nBytesTotal,train_imagesfile.read(nBytesTotal))).reshape((nImg,nR,nC))\n",
    "\n",
    "X = (1/255)*images_array.reshape(nImg,nR*nC)\n",
    "\n",
    "print(\"Size images_array :\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magic Number :  2049\n",
      "num of images :  60000\n",
      "Size images_array :  60000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b7923df3ae78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinearsum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_layer_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-8a36d4277737>\u001b[0m in \u001b[0;36mL_layer_network\u001b[1;34m(X, Y, learningrate, layer_dims, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mA_prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mactivation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinearsum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Z'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Relu\"\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#Calculation of activaion output of hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-16c9dd5873c6>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[1;34m(A_prev, W, b, activation_type)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mactivation_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Relu\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "layer_dims = [np.squeeze(np.shape(X[0])),20,10,10]\n",
    "learningrate = 1\n",
    "print_cost=True\n",
    "\n",
    "#Label Data(MNIST)\n",
    "train_labelfile = open(filename['labels'],'rb')\n",
    "MagicNumber = st.unpack('>I',train_labelfile.read(4))[0]\n",
    "nlabel = st.unpack('>I',train_labelfile.read(4))[0] #num of labels\n",
    "print(\"Magic Number : \", MagicNumber)\n",
    "print(\"num of images : \", nlabel)\n",
    "label_array = np.zeros((nlabel))\n",
    "\n",
    "nBytesTotal = nlabel*1 #since each pixel data is 1 byte\n",
    "label_array = np.asarray(st.unpack('>'+'B'*nBytesTotal,train_labelfile.read(nBytesTotal))).reshape((nlabel))\n",
    "\n",
    "print(\"Size images_array : \",len(label_array))\n",
    "\n",
    "Y=np.zeros((len(label_array),10))\n",
    "for i in range (0,len(label_array)):\n",
    "    for j in range (0,10):\n",
    "        if label_array[i]==j:\n",
    "            Y[i][j] = 1\n",
    "        else:\n",
    "            Y[i][j] = 0\n",
    "\n",
    "parameters,activation,linearsum = L_layer_network(X.T, Y.T, learningrate, layer_dims, 5000, print_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodes the output that the trained model predicts on the testing set\n",
    "def Output(A2):\n",
    "    key = A2[0][0]\n",
    "    p = 0\n",
    "    for i in range (0,10):\n",
    "        if A2[i][0]>=key:\n",
    "                p = i\n",
    "                key = A2[i][0]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the accuracy the trained model is able to achieve\n",
    "def accuracy_predictor(out, label_array):\n",
    "    fav = 0\n",
    "    for i in range(0,51):\n",
    "        if(out[i]==label_array[i]):\n",
    "            fav = fav + 1\n",
    "    accuracy = fav/51*100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeltest(X,parameters,activation,linearsum,out):\n",
    "    L = len(parameters) // 2 \n",
    "    A = X\n",
    "    for l in range(1, L):\n",
    "            A_prev = A\n",
    "            activation['A'+str(l)], linearsum['Z'+str(l)] = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b' + str(l)], \"tanh\")   #Calculation of activaion output of hidden layer\n",
    "            A = activation['A'+str(l)]\n",
    "        \n",
    "    activation['A'+str(L)], linearsum['Z'+str(L)] = linear_activation_forward(A, parameters['W'+str(L)], parameters['b' + str(L)], \"Sigmoid\")        #Calculation of activation output of output layer\n",
    "    \n",
    "    A = Output(activation['A'+str(L)])\n",
    "    out.append(A)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X=np.zeros(shape=(784,1))\n",
    "out = []\n",
    "filename = {'images' : 'E:/Handwritten digit recognisation/dataset/t10k-images-idx3-ubyte' ,'labels' : 'E:/Handwritten digit recognisation/dataset/t10k-labels-idx1-ubyte'}\n",
    "Test_imagesfile = open(filename['images'],'rb')\n",
    "\n",
    "MagicNumber = st.unpack('>I',Test_imagesfile.read(4))[0]\n",
    "nImg = st.unpack('>I',Test_imagesfile.read(4))[0] #num of images\n",
    "nR = st.unpack('>I',Test_imagesfile.read(4))[0] #num of rows\n",
    "nC = st.unpack('>I',Test_imagesfile.read(4))[0] #num of column\n",
    "print(\"Magic Number : \", MagicNumber)\n",
    "print(\"num of images : \", nImg)\n",
    "print(\"num of rows : \", nR)\n",
    "print(\"num of column : \", nC)\n",
    "\n",
    "Tex = np.zeros((nImg,nR,nC))\n",
    "\n",
    "nBytesTotal = nImg*nR*nC*1 #since each pixel data is 1 byte\n",
    "Tex = (1/255)*np.asarray(st.unpack('>'+'B'*nBytesTotal,Test_imagesfile.read(nBytesTotal))).reshape((nImg,nR*nC))\n",
    "\n",
    "print(\"Size images_array : \",Tex.shape)\n",
    "\n",
    "Test_labelfile = open(filename['labels'],'rb')\n",
    "\n",
    "MagicNumber = st.unpack('>I',Test_labelfile.read(4))[0]\n",
    "nlabel = st.unpack('>I',Test_labelfile.read(4))[0] #num of labels\n",
    "print(\"Magic Number : \", MagicNumber)\n",
    "print(\"num of images : \", nlabel)\n",
    "\n",
    "label_arrayT = np.zeros((nlabel))\n",
    "\n",
    "nBytesTotal = nlabel*1 #since each pixel data is 1 byte\n",
    "label_arrayT= np.asarray(st.unpack('>'+'B'*nBytesTotal,Test_labelfile.read(nBytesTotal))).reshape((nlabel))\n",
    "\n",
    "\n",
    "print(\"Size images_array : \",len(label_arrayT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,len(label_arrayT)):\n",
    "    for i in range(0,784):\n",
    "        Train_X[i][0] = Tex[j][i]\n",
    "    out = modeltest(Train_X, parameters,activation,linearsum, out)\n",
    "    \n",
    "accuracy = accuracy_predictor(out,label_arrayT)    \n",
    "print(\"A :\",out, end=\" \")\n",
    "print(\"\\n\\naccuracy = \", accuracy, end=\"%\")\n",
    "print(\"\\nout\",len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
